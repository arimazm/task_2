{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.lm.models import LanguageModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 300 songs lyrics in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_Eminem.json'\n",
    "with open(file) as train_file:\n",
    "    dict_eminem = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminem_songs = dict_eminem.get('songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminem_lyrics = []\n",
    "for i in range(0,len(eminem_songs)):\n",
    "    dicts = eminem_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    eminem_lyrics.append(song_lyric) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminem = pd.DataFrame(eminem_lyrics) \n",
    "eminem = eminem.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminem = eminem.replace(to_replace = '\\n', value = ' ', regex = True) #replacing all new line characters\n",
    "eminem[0] = eminem[0].str.replace('[^\\w\\s]','') #replacing all things like '[Intro]'\n",
    "eminem[0] = eminem[0].str.lower() # changing everything to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminem_lyrics = eminem[0].to_list() # creating tokens from all the songs, feel like it's redundant as I do it again later\n",
    "tokens1 = []\n",
    "for i in range(len(eminem_lyrics)):\n",
    "    tokens = nltk.word_tokenize(eminem_lyrics[i])\n",
    "    tokens1.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for sublist in tokens1:\n",
    "    for token in sublist:\n",
    "        tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234205"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(eminem, test_size = 0.1, random_state = 123) #splitting dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tr = list(train[0].apply(nltk.word_tokenize)) # tokenizing both sets\n",
    "tokens_te = list(test[0].apply(nltk.word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_test = [] # creating test tokens\n",
    "for sublist in tokens_te:\n",
    "    for token in sublist:\n",
    "        tokens_test.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = [] # creating train tokens\n",
    "for sublist in tokens_tr:\n",
    "    for token in sublist:\n",
    "        tokens_train.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22333 211872\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_test), len(tokens_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to thank github and Dasha Dobrego for these chunks\n",
    "https://github.com/nltk/nltk/pull/2363/commits/ce74e449dc9526e19596b1c4a9c510bbb35812cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedLanguageModel(LanguageModel):\n",
    "    \"\"\"Logic common to all interpolated language models.\n",
    "    The idea to abstract this comes from Chen & Goodman 1995.\n",
    "    Do not instantiate this class directly!\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing_cls, order, **kwargs):\n",
    "        assert issubclass(smoothing_cls, Smoothing)\n",
    "        params = kwargs.pop(\"params\", {})\n",
    "        super().__init__(order, **kwargs)\n",
    "        self.estimator = smoothing_cls(self.vocab, self.counts, **params)\n",
    "\n",
    "    def unmasked_score(self, word, context=None):\n",
    "        if not context:\n",
    "                return self.estimator.unigram_score(word)\n",
    "        if not self.counts[context]:\n",
    "#This conversation was marked as resolved by stevenbird  Show conversation\n",
    "             # It can also happen that we have no data for this context.\n",
    "             # In that case we defer to the lower-order ngram.\n",
    "             # This is the same as setting alpha to 0 and gamma to 1.\n",
    "             return self.unmasked_score(word, context[1:])\n",
    "        alpha, gamma = self.estimator.alpha_gamma(word, context)\n",
    "        return alpha + gamma * self.unmasked_score(word, context[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_non_zero_vals(dictionary):\n",
    "    return sum(1.0 for c in dictionary.values() if c > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.models import Smoothing\n",
    "class KneserNey(Smoothing):\n",
    "    def __init__(self, vocabulary, counter, discount=0.1, **kwargs):\n",
    "        super(KneserNey, self).__init__(vocabulary, counter, *kwargs)\n",
    "        super().__init__(vocabulary, counter, **kwargs)\n",
    "        self.discount = discount\n",
    "\n",
    "    def unigram_score(self, word):\n",
    "        return 1.0 / len(self.vocab)\n",
    "\n",
    "    def alpha_gamma(self, word, context):\n",
    "        prefix_counts = self.counts[context]\n",
    "        prefix_total_ngrams = prefix_counts.N()\n",
    "        alpha = max(prefix_counts[word] - self.discount, 0.0) / prefix_total_ngrams\n",
    "        gamma = self.discount * _count_non_zero_vals(prefix_counts) / prefix_total_ngrams\n",
    "        return alpha, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneserNeyInterpolated(InterpolatedLanguageModel):\n",
    "    def __init__(self, order, discount=0.1, **kwargs):\n",
    "        super().__init__(KneserNey, order, params={\"discount\": discount}, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fivegram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedLine5 = [list(pad_both_ends(tokens_train, n=5))]\n",
    "train5, vocab5 = padded_everygram_pipeline(5, paddedLine5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_fivemod = KneserNeyInterpolated(5, discount = 0.99) \n",
    "lyrics_fivemod.fit(train5, vocab5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fivegrams = ngrams(tokens_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "798.1132655750536"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_fivemod.perplexity(test_fivegrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedLine4 = [list(pad_both_ends(tokens_train, n=4))]\n",
    "train4, vocab4 = padded_everygram_pipeline(4, paddedLine4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_fourmod = KneserNeyInterpolated(4, discount = 0.99) \n",
    "lyrics_fourmod.fit(train4, vocab4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fourgrams = ngrams(tokens_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801.465368787004"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_fourmod.perplexity(test_fourgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedLine = [list(pad_both_ends(tokens_train, n=3))]\n",
    "train3, vocab3 = padded_everygram_pipeline(3, paddedLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_trimod = KneserNeyInterpolated(3, discount = 0.99) \n",
    "lyrics_trimod.fit(train3, vocab3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 635637 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_trimod.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trigrams = ngrams(tokens_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813.716714003829"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_trimod.perplexity(test_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedLine2 = [list(pad_both_ends(tokens_train, n=2))]\n",
    "train2, vocab2 = padded_everygram_pipeline(2, paddedLine2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_bimod = KneserNeyInterpolated(2, discount = 0.99) \n",
    "lyrics_bimod.fit(train2, vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bigrams = ngrams(tokens_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 423751 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_bimod.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891.4658919531347"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_bimod.perplexity(test_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the 5gram models has shown the best results, but they are still pretty bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there's not enough data, so I'll add more songs of other rappers. Added 800 songs in total. \n",
    "\n",
    "Addendum: added another 1k songs after the first 800 didnt do much\n",
    "\n",
    "Addendum 2.0: added around 1.5k more because the previous 1.8k weren't enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_KendrickLamar.json'\n",
    "with open(file) as train_file:\n",
    "    dict_kendrick = json.load(train_file)\n",
    "    \n",
    "kendrick_songs = dict_kendrick.get('songs')\n",
    "\n",
    "kendrick_lyrics = []\n",
    "for i in range(0,len(kendrick_songs)):\n",
    "    dicts = kendrick_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    kendrick_lyrics.append(song_lyric)\n",
    "\n",
    "kendrick = pd.DataFrame(kendrick_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_KanyeWest.json'\n",
    "with open(file) as train_file:\n",
    "    dict_kanye = json.load(train_file)\n",
    "\n",
    "kanye_songs = dict_kanye.get('songs')\n",
    "\n",
    "kanye_lyrics = []\n",
    "for i in range(0,len(kanye_songs)):\n",
    "    dicts = kanye_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    kanye_lyrics.append(song_lyric) \n",
    "\n",
    "kanye = pd.DataFrame(kanye_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_2Pac.json'\n",
    "with open(file) as train_file:\n",
    "    dict_2pac = json.load(train_file)\n",
    "    \n",
    "dict_2pac_songs = dict_2pac.get('songs')\n",
    "\n",
    "pac_lyrics = []\n",
    "for i in range(0,len(dict_2pac_songs)):\n",
    "    dicts = dict_2pac_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    pac_lyrics.append(song_lyric)\n",
    "    \n",
    "pac = pd.DataFrame(pac_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_JAYZ.json'\n",
    "with open(file) as train_file:\n",
    "    dict_jayz = json.load(train_file)\n",
    "    \n",
    "jay_songs = dict_jayz.get('songs')\n",
    "\n",
    "jay_lyrics = []\n",
    "for i in range(0,len(jay_songs)):\n",
    "    dicts = jay_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    jay_lyrics.append(song_lyric)\n",
    "    \n",
    "jay = pd.DataFrame(jay_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_LilWayne.json'\n",
    "with open(file) as train_file:\n",
    "    dict_wayne = json.load(train_file)\n",
    "    \n",
    "wayne_songs = dict_wayne.get('songs')\n",
    "\n",
    "wayne_lyrics = []\n",
    "for i in range(0,len(wayne_songs)):\n",
    "    dicts = wayne_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    wayne_lyrics.append(song_lyric)\n",
    "    \n",
    "wayne = pd.DataFrame(wayne_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_SnoopDogg.json'\n",
    "with open(file) as train_file:\n",
    "    dict_snoop = json.load(train_file)\n",
    "    \n",
    "snoop_songs = dict_snoop.get('songs')\n",
    "\n",
    "snoop_lyrics = []\n",
    "for i in range(0,len(snoop_songs)):\n",
    "    dicts = snoop_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    snoop_lyrics.append(song_lyric)\n",
    "    \n",
    "snoop = pd.DataFrame(snoop_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_50Cent.json'\n",
    "with open(file) as train_file:\n",
    "    dict_fifty = json.load(train_file)\n",
    "    \n",
    "fifty_songs = dict_fifty.get('songs')\n",
    "\n",
    "fifty_lyrics = []\n",
    "for i in range(0,len(fifty_songs)):\n",
    "    dicts = fifty_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    fifty_lyrics.append(song_lyric)\n",
    "    \n",
    "fifty = pd.DataFrame(fifty_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_Dr.Dre.json'\n",
    "with open(file) as train_file:\n",
    "    dict_dre = json.load(train_file)\n",
    "    \n",
    "dre_songs = dict_dre.get('songs')\n",
    "\n",
    "dre_lyrics = []\n",
    "for i in range(0,len(dre_songs)):\n",
    "    dicts = dre_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    dre_lyrics.append(song_lyric)\n",
    "    \n",
    "dre = pd.DataFrame(dre_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_TheNotoriousB.I.G..json'\n",
    "with open(file) as train_file:\n",
    "    dict_biggy = json.load(train_file)\n",
    "    \n",
    "biggy_songs = dict_biggy.get('songs')\n",
    "\n",
    "biggy_lyrics = []\n",
    "for i in range(0,len(biggy_songs)):\n",
    "    dicts = biggy_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    biggy_lyrics.append(song_lyric)\n",
    "    \n",
    "biggy = pd.DataFrame(biggy_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_WizKhalifa.json'\n",
    "with open(file) as train_file:\n",
    "    dict_wiz = json.load(train_file)\n",
    "    \n",
    "wiz_songs = dict_wiz.get('songs')\n",
    "\n",
    "wiz_lyrics = []\n",
    "for i in range(0,len(wiz_songs)):\n",
    "    dicts = wiz_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    wiz_lyrics.append(song_lyric)\n",
    "    \n",
    "wiz = pd.DataFrame(wiz_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_AAPRocky.json'\n",
    "with open(file) as train_file:\n",
    "    dict_rocky = json.load(train_file)\n",
    "    \n",
    "rocky_songs = dict_rocky.get('songs')\n",
    "\n",
    "rocky_lyrics = []\n",
    "for i in range(0,len(rocky_songs)):\n",
    "    dicts = rocky_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    rocky_lyrics.append(song_lyric)\n",
    "    \n",
    "rocky = pd.DataFrame(rocky_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_EazyE.json'\n",
    "with open(file) as train_file:\n",
    "    dict_eazy = json.load(train_file)\n",
    "    \n",
    "eazy_songs = dict_eazy.get('songs')\n",
    "\n",
    "eazy_lyrics = []\n",
    "for i in range(0,len(eazy_songs)):\n",
    "    dicts = eazy_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    eazy_lyrics.append(song_lyric)\n",
    "    \n",
    "eazy = pd.DataFrame(eazy_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_TechN9ne.json'\n",
    "with open(file) as train_file:\n",
    "    dict_tech = json.load(train_file)\n",
    "    \n",
    "tech_songs = dict_tech.get('songs')\n",
    "\n",
    "tech_lyrics = []\n",
    "for i in range(0,len(tech_songs)):\n",
    "    dicts = tech_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    tech_lyrics.append(song_lyric)\n",
    "    \n",
    "tech = pd.DataFrame(tech_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_OutKast.json'\n",
    "with open(file) as train_file:\n",
    "    dict_outkast = json.load(train_file)\n",
    "    \n",
    "outkast_songs = dict_outkast.get('songs')\n",
    "\n",
    "outkast_lyrics = []\n",
    "for i in range(0,len(outkast_songs)):\n",
    "    dicts = outkast_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    outkast_lyrics.append(song_lyric)\n",
    "    \n",
    "outkast = pd.DataFrame(outkast_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_ChancetheRapper.json'\n",
    "with open(file) as train_file:\n",
    "    dict_chance = json.load(train_file)\n",
    "    \n",
    "chance_songs = dict_chance.get('songs')\n",
    "\n",
    "chance_lyrics = []\n",
    "for i in range(0,len(chance_songs)):\n",
    "    dicts = chance_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    chance_lyrics.append(song_lyric)\n",
    "    \n",
    "chance = pd.DataFrame(chance_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_IceCube.json'\n",
    "with open(file) as train_file:\n",
    "    dict_cube = json.load(train_file)\n",
    "    \n",
    "cube_songs = dict_cube.get('songs')\n",
    "\n",
    "cube_lyrics = []\n",
    "for i in range(0,len(cube_songs)):\n",
    "    dicts = cube_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    cube_lyrics.append(song_lyric)\n",
    "    \n",
    "cube = pd.DataFrame(cube_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_MachineGunKelly.json'\n",
    "with open(file) as train_file:\n",
    "    dict_kelly = json.load(train_file)\n",
    "    \n",
    "kelly_songs = dict_kelly.get('songs')\n",
    "\n",
    "kelly_lyrics = []\n",
    "for i in range(0,len(kelly_songs)):\n",
    "    dicts = kelly_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    kelly_lyrics.append(song_lyric)\n",
    "    \n",
    "kelly = pd.DataFrame(kelly_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_Logic.json'\n",
    "with open(file) as train_file:\n",
    "    dict_logic = json.load(train_file)\n",
    "    \n",
    "logic_songs = dict_logic.get('songs')\n",
    "\n",
    "logic_lyrics = []\n",
    "for i in range(0,len(logic_songs)):\n",
    "    dicts = logic_songs[i]\n",
    "    song_lyric = dicts.get('lyrics')\n",
    "    logic_lyrics.append(song_lyric)\n",
    "    \n",
    "logic = pd.DataFrame(logic_lyrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lyrics = pd.concat([kendrick, kanye, pac, jay, wayne, snoop, fifty, dre, biggy, wiz, logic, kelly, cube, chance, outkast, tech, eazy, rocky])\n",
    "total_lyrics = total_lyrics.replace(to_replace ='[\\(\\[].*?[\\)\\]]', value = ' ', regex = True) #replacing all things like '[Intro]'\n",
    "total_lyrics = total_lyrics[total_lyrics[0].notna()] #dropping na rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nNobody pray for me\\nIt's been that day for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n \\nIf Pirus and Crips all got along\\nThey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n \\nPour up  , head shot  \\nSit down  , st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nI got, I got, I got, I got—\\nLoyalty, got r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n \\nUh, me and my niggas tryna get it, ya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>\\n            Lyrics for this song h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>\\n            Lyrics for this song h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>\\n            Lyrics for this song h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>\\n            Lyrics for this song h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Lyrics from Snippet\\nThey all collecting my re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3418 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0     \\nNobody pray for me\\nIt's been that day for ...\n",
       "1     \\n\\n \\nIf Pirus and Crips all got along\\nThey...\n",
       "2     \\n\\n \\nPour up  , head shot  \\nSit down  , st...\n",
       "3     \\nI got, I got, I got, I got—\\nLoyalty, got r...\n",
       "4     \\n\\n \\nUh, me and my niggas tryna get it, ya ...\n",
       "..                                                 ...\n",
       "200            \\n            Lyrics for this song h...\n",
       "201            \\n            Lyrics for this song h...\n",
       "202            \\n            Lyrics for this song h...\n",
       "203            \\n            Lyrics for this song h...\n",
       "205  Lyrics from Snippet\\nThey all collecting my re...\n",
       "\n",
       "[3418 rows x 1 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lyrics = total_lyrics.replace(to_replace = '\\n', value = ' ', regex = True)\n",
    "total_lyrics[0] = total_lyrics[0].str.replace('[^\\w\\s]','')\n",
    "total_lyrics[0] = total_lyrics[0].str.lower()\n",
    "#removed punctuation and newline and made lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobody pray for me its been that day for me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if pirus and crips all got along theyd pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pour up   head shot   sit down   stand up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i got i got i got i got loyalty got royalty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uh me and my niggas tryna get it ya bish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>lyrics for this song ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>lyrics for this song ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>lyrics for this song ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>lyrics for this song ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>lyrics from snippet they all collecting my res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3418 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0      nobody pray for me its been that day for me ...\n",
       "1         if pirus and crips all got along theyd pr...\n",
       "2         pour up   head shot   sit down   stand up...\n",
       "3      i got i got i got i got loyalty got royalty ...\n",
       "4         uh me and my niggas tryna get it ya bish ...\n",
       "..                                                 ...\n",
       "200                         lyrics for this song ha...\n",
       "201                         lyrics for this song ha...\n",
       "202                         lyrics for this song ha...\n",
       "203                         lyrics for this song ha...\n",
       "205  lyrics from snippet they all collecting my res...\n",
       "\n",
       "[3418 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lyrics = total_lyrics[0].to_list()\n",
    "# creating tokens from all the new data\n",
    "tokens2 = []\n",
    "for i in range(len(final_lyrics)):\n",
    "    tokens = nltk.word_tokenize(final_lyrics[i])\n",
    "    tokens2.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3418"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = []\n",
    "for sublist in tokens2:\n",
    "    for token in sublist:\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1878309"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2090181"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = new_tokens + tokens_train\n",
    "len(all_tokens)\n",
    "# adding tokens from eminem to have the final number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think that I need to check bigram models, because so far they haven't shown good results. I'll add a sixgram model instead, to see how the perplexity changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_Line6 = [list(pad_both_ends(all_tokens, n=6))]\n",
    "train26, vocab26 = padded_everygram_pipeline(6, padded_Line6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_sixmod1 = KneserNeyInterpolated(6, discount = 0.99) \n",
    "lyrics_sixmod1.fit(train26, vocab26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sixgrams = ngrams(tokens_test, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563.0705670276024"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_sixmod1.perplexity(test_sixgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fivegrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_Line5 = [list(pad_both_ends(all_tokens, n=5))]\n",
    "train25, vocab25 = padded_everygram_pipeline(5, padded_Line5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_fivemod1 = KneserNeyInterpolated(5, discount = 0.99) \n",
    "lyrics_fivemod1.fit(train25, vocab25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fivegrams = ngrams(tokens_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_fivemod1.logscore('Frederic', 'loves to eat good food'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563.6897657510704"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_fivemod1.perplexity(test_fivegrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_Line4 = [list(pad_both_ends(all_tokens, n=4))]\n",
    "train24, vocab24 = padded_everygram_pipeline(4, padded_Line4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_fourmod1 = KneserNeyInterpolated(4, discount = 0.99) \n",
    "lyrics_fourmod1.fit(train24, vocab24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fourgrams = ngrams(tokens_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563.4352503769684"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_fourmod1.perplexity(test_fourgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_Line3 = [list(pad_both_ends(all_tokens, n=3))]\n",
    "train23, vocab23 = padded_everygram_pipeline(3, padded_Line3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_trimod1 = KneserNeyInterpolated(3, discount = 0.99) \n",
    "lyrics_trimod1.fit(train23, vocab23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trigrams = ngrams(tokens_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568.2308393118316"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_trimod1.perplexity(test_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, another 3k songs only made perplexity 300 smaller, so assuming that the math stays the same, I'll need another 5k to make it 0, and I think this is a very optimistic assumption on my part.\n",
    "\n",
    "I will just go on working with the models that I have now. For text generating, I'll choose 4- and 6-gram models, because they have the best results among the ones presented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed):\n",
    "    content = []\n",
    "    period = '.' # to finish the sentences and make working with them later on easier\n",
    "    words = 0\n",
    "    while words < num_words:\n",
    "        for token in model.generate(num_words, random_seed=random_seed):\n",
    "            if token == '<s>':\n",
    "                continue\n",
    "            if token == '</s>':\n",
    "                content.append(period)\n",
    "                break\n",
    "            content.append(token)\n",
    "        words += 1\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, num_sents): #, random_seed #generate text with the sentences from above\n",
    "    text = []\n",
    "    sentences = 0\n",
    "    while sentences < num_sents:\n",
    "        for i in range(random.randint(0, 1000)):\n",
    "            sentence = generate_sent(model, num_words = 20, random_seed = i)\n",
    "        sentences += 1\n",
    "        text.append(sentence)\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-44f4fa5e05a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlyrics_fourmod1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-60608feffb6c>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, num_sents)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_sents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-766a1f9a2cf3>\u001b[0m in \u001b[0;36mgenerate_sent\u001b[0;34m(model, num_words, random_seed)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'<s>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[1;32m    247\u001b[0m                     \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                     \u001b[0mtext_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_seed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                     \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                 )\n\u001b[1;32m    251\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             return _weighted_choice(\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             )\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# build up text one word at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             return _weighted_choice(\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             )\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# build up text one word at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \"\"\"\n\u001b[1;32m    142\u001b[0m         return self.unmasked_score(\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         )\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ac46aa7a38d7>\u001b[0m in \u001b[0;36munmasked_score\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munmasked_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigram_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#This conversation was marked as resolved by stevenbird  Show conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d7c4814e04d8>\u001b[0m in \u001b[0;36munigram_score\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munigram_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malpha_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/vocabulary.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m\"\"\"Computing size of vocabulary reflects the cutoff.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/vocabulary.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m\"\"\"Computing size of vocabulary reflects the cutoff.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/vocabulary.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    222\u001b[0m         vocabulary.\"\"\"\n\u001b[1;32m    223\u001b[0m         return chain(\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_text(lyrics_fourmod1, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to generate text using the functions from above, but it was taking too long to generate even 20 sentences (think about a day long, and then at 10-hour mark I accidentally interrupted kernel, so the decision was taken out of my hands, I guess), so I'll use someone else's program to generate song. \n",
    "\n",
    "My biggest thanks to Pratap Vardhan at kaggle.com for this program. \n",
    "https://www.kaggle.com/pratapvardhan/kanye-lyrics-eda-song-generator-topic-modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_total = pd.concat([total_lyrics, eminem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobody pray for me its been that day for me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if pirus and crips all got along theyd pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pour up   head shot   sit down   stand up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i got i got i got i got loyalty got royalty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uh me and my niggas tryna get it ya bish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>hi my name is what my name is who my name is c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>are we supposed to shut up or talkill cut your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>kick a hypest lyric every time i run up on a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>come on put me in my place come on put me in m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>yo brabbit this silly rabbit dont like lettuce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3716 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0      nobody pray for me its been that day for me ...\n",
       "1         if pirus and crips all got along theyd pr...\n",
       "2         pour up   head shot   sit down   stand up...\n",
       "3      i got i got i got i got loyalty got royalty ...\n",
       "4         uh me and my niggas tryna get it ya bish ...\n",
       "..                                                 ...\n",
       "295  hi my name is what my name is who my name is c...\n",
       "296  are we supposed to shut up or talkill cut your...\n",
       "297  kick a hypest lyric every time i run up on a t...\n",
       "298  come on put me in my place come on put me in m...\n",
       "299  yo brabbit this silly rabbit dont like lettuce...\n",
       "\n",
       "[3716 rows x 1 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine generated lyrics using Markov\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MarkovRachaita:\n",
    "    def __init__(self, corpus='', order=2, length=8):\n",
    "        self.order = order\n",
    "        self.length = length\n",
    "        self.words = re.findall(\"[a-z']+\", corpus.lower())\n",
    "        self.states = defaultdict(list)\n",
    "\n",
    "        for i in range(len(self.words) - self.order):\n",
    "            self.states[tuple(self.words[i:i + self.order])].append(self.words[i + order])\n",
    "\n",
    "    def gen_sentence(self, length=8, startswith=None):\n",
    "        terms = None\n",
    "        if startswith:\n",
    "            start_seed = [x for x in self.states.keys() if startswith in x]\n",
    "            if start_seed:\n",
    "                terms = list(start_seed[0])\n",
    "        if terms is None:\n",
    "            start_seed = random.randint(0, len(self.words) - self.order)\n",
    "            terms = self.words[start_seed:start_seed + self.order]\n",
    "\n",
    "        for _ in range(length):\n",
    "            terms.append(random.choice(self.states[tuple(terms[-self.order:])]))\n",
    "\n",
    "        return ' '.join(terms)\n",
    "\n",
    "    def gen_song(self, lines=10, length=8, length_range=None, startswith=None):\n",
    "        song = []\n",
    "        if startswith:\n",
    "            song.append(self.gen_sentence(length=length, startswith=startswith))\n",
    "            lines -= 1\n",
    "        for _ in range(lines):\n",
    "            sent_len = random.randint(*length_range) if length_range else length\n",
    "            song.append(self.gen_sentence(length=sent_len))\n",
    "        return '\\n'.join(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap = MarkovRachaita(corpus=' '.join(total_total[0]))\n",
    "rap_hit = rap.gen_song(lines=20, length_range=[5, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rap_hit = rap_hit.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is amazing barely scratching the surface they come\n",
      "soft porn before it fades into black and yellow\n",
      "jodye tell these hoes alls we know we need a one girls\n",
      "me do this you go one more\n",
      "got greed khop check this bitches aint shit in the same thang thang\n",
      "gunit i murk you my nigga come who crazy hot im snoop dogg im bought up and\n",
      "some bad bitches thats my trippy kit yea weed pills and that cow\n",
      "knowing junkies on the mic you see im a stain on\n",
      "on like a sony while the demons let em know im roguish surrounded by\n",
      "my job here isnt done cause they dont understand me but youre still a rider nigga\n",
      "you are now blown all over our necks like were supposed to be\n",
      "im not finna fold uh if you sound frantic i hear the voice the west side\n",
      "day jumpin the gun to the greatness or die\n",
      "in your life yet every time we had to do what i want to say we aint\n",
      "spell running free even though you the middle\n",
      "gang tight and the pentecostal why yall even\n",
      "just felt like you just act hollywood like they\n",
      "got an ashtray where did my head that only god only knows what\n",
      "the rats kitten scratch i love you\n",
      "ready yeah sing it uh you feel never squealed like that its okay\n"
     ]
    }
   ],
   "source": [
    "for line in rap_hit:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think that this makes any sense, but beggars cant be choosers, and I didn't use any of my awesome Ngrams, so that's a shame. \n",
    "\n",
    "Suggestions for improvement:\n",
    "1. try better models next time\n",
    "2. make sure not to interrupt anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "1. https://www.kaggle.com/alvations/n-gram-language-model-with-nltk\n",
    "2. https://github.com/nltk/nltk/pull/2363/commits/ce74e449dc9526e19596b1c4a9c510bbb35812cc\n",
    "3. https://www.kaggle.com/pratapvardhan/kanye-lyrics-eda-song-generator-topic-modelling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
